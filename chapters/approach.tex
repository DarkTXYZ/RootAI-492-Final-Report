\chapter{\ifproject%
\ifenglish Project Structure and Methodology\else โครงสร้างและขั้นตอนการทำงาน\fi
\else%
\ifenglish Project Structure\else โครงสร้างของโครงงาน\fi
\fi
}

The two main components of this project: \RootOurs \ and \RootAI.

% ในบทนี้จะกล่าวถึงหลักการ และการออกแบบระบบ

\makeatletter

% \renewcommand\section{\@startsection {section}{1}{\z@}%
%                                    {13.5ex \@plus -1ex \@minus -.2ex}%
%                                    {2.3ex \@plus.2ex}%
%                                    {\normalfont\large\bfseries}}

\makeatother
%\vspace{2ex}
% \titleformat{\section}{\normalfont\bfseries}{\thesection}{1em}{}
% \titlespacing*{\section}{0pt}{10ex}{0pt}

\section{\RootOurs}
\RootOurs \ is the game environment that mimics \RootV in a game of \Marquise \ versus \Eyrie. It has the following features:
\begin{itemize}
  \item Simulate the current state of the game
  \item Generate all possible actions for the current player at the current state of the game
  \item Change state according to the selected action 
  \begin{itemize}
    \item With option for whether the simulation will random the non-deterministic actions by itself or the player can input a specific outcome value for the non-deterministic actions.
  \end{itemize}
\end{itemize}


% From P Khem's 
% This module enables other services to communicate with the dictionary. It performs querying
% words and adding new words on the request. There is only a single dictionary management
% system per back-end service. The system performs word querying and word modifying in
% the database, which the back-end service cannot do directly.
\section{\RootAI}
\RootAI \ is the framework that train AI agents in \RootOurs \ instance's environment. Due to a shortage of time and players, we use the multi-agent learning approach, agents competes against agents, rather than actual players.

There are 3 agent implementation methods: Random decision, Monte Carlo tree search, and Reinforcement learning neural network.

\subsection{Random decision}
The agent constructed using the random decision method will uniformly select an action from all legal actions at the current state. The list of legal actions has a \gls{discrete-uniform-distribution}, with each action having an equal likelihood of being chosen. % TODO: cite Wiki: discrete uniform distribution

% The agent built with random decision method will randomly select actions where all action has a uniform probability to be picked.

\subsection{Monte Carlo tree search (MCTS)} % TODO: add problem with current MCTS and Root
The agent will generate a tree search at the current state, determine which action has the highest probability of winning the game, and take that action. The winning probability of each action will be calculated using the MCTS approach. 

Because of the nondetermism mechanism in games (rolling dice, drawing random cards), we use a specific variant of MCTS, Open-Loop MCTS.

\subsection{Reinforcement learning neural network (RLNN)} % TODO
The agent constructed using RLNN will use RNN type model, taking the current state, past states, and past actions as input, and outputs the probability of choosing each action from all legal actions in the current state. Then calculate the reward of that action using the reward function (which is yet to be defined). Adjust the neural network according to the reward, and adjust the agent's current state to the new state in the feedback. % unconfirmed


\begin{enumerate}
  \item Agent: the player that's playing as \Marquise \ faction or \Eyrie \ faction
  \item Environment: the map, opponent's state and action, draw pile, etc. in \RootOurs % not sure
  \item State: all game components that belonged to the player
  \item Reward: victory points earned, getting a win, or taking hold of clearing with matching suit to the played dominance card, all combined together through the reward function 
  \item Policy: the neural network
\end{enumerate}

% TODO: add evaluation method