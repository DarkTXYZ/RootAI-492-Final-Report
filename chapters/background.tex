\chapter{\ifenglish Background Knowledge and Theory\else ทฤษฎีที่เกี่ยวข้อง\fi}

In order to create the AI, we first need to implement \RootOurs{}. This requires knowledge of game programming fundamentals and extensive knowledge of the rules of \RootB{}. However, there are minor rule difference of \RootV{} and \RootB{}, which for us to create an AI to compete with \RootV{}'s AI, we also need to find and implement these minor differences into our \RootOurs{}.

Next step is to implement \RootAI{}, the framework for running \RootOurs{} and training AI agents. There are two AI agent implementation methods that requires additional self study: Monte Carlo tree search and reinforcement learning neural network.

\section{Root}
\subsection{Game Theory}
Root is an \textbf{asymetric game}: a game where each player starts with different setups, gameplay mechanics, etc. Although Root's base actions such as move and battle are the same for every faction, each faction starts differently and each has its own unique mechanics and rules. Thus, Root sits closer to the asymmetric side in the symmetric--asymmetric spectrum.\\
Examples of symmetric games are Chess, Monopoly, Prisoner's Dilemma, etc.\\
Examples of other asymmetric games are Tapestry, Dune, Cthulhu Wars, etc.

Root is an \textbf{imperfect information game}: a game where there is information hidden from the player. Most elements of Root, such as warrior placement, building count, etc., are laid out for all players to see. But cards on the player's hand are only visible to that player, i.e., other players' cards in hand are the hidden information in this game.

Root is a \textbf{non-deterministic} game: a game where there's some element of randomness involved, such as rolling dice or shuffling cards. Root has both, rolling dice in battle and drawing cards from a shuffled draw-pile.

\subsection{Brief Rules of Root} \label{brief-rules-of-root}
Root is a 2-4 player turn-based game, where you play as one of the four factions, fighting against other factions to be the ruler of woodland.

The rule is simple: the player who gets 30 victory points or plays and completes the dominance card (the card that change the endgame winning condition) first wins. The players play on the map of Woodland (Fig 2.1), which contains 12 clearings, each with different suits and different number of slots for building. There are also paths that connect those clearings for moving warriors from one to another clearing.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{root_game_board.jpg}
  \end{center}
  \caption{Woodland, Root's game board}
  \label{fig:root-game-board}
\end{figure}

Each player will take his turn throughout three phases: birdsong, daylight, and evening. The player will be instructed on what he must do and what actions he can do in each phase. Also, each faction will have special abilities that can be used during their turn. Instructions and actions will vary from faction to faction, but there are actions that are shared by all factions, called key actions. There are three key actions:

\begin{enumerate}
  \item Move: Take any number of your warriors from one clearing and move them to one adjacent clearing.
  \item Craft: Using crafting pieces you have to craft a card from your hand. After crafting, gain benefits or effects on the crafted card.
  \item Battle: Choose a clearing where you have warriors as the clearing of battle. You are the attacker. Choose another player in the clearing of battle to be the defender. Roll two dice. The attacker deals hits equal to the higher roll, and the defender deals hits equal to the lower roll. Both players deal hits simultaneously and remove pieces equal to the number of hits they receive.
\end{enumerate}

Other than key actions, there will be some specific actions belonging to each faction. We will describe them in detail with an explanation of how each phase of each faction works in Table (2.1) and (2.2).

Some of the rules will not be explained here, as they are not crucial and essential. More rules can be found at here \cite{RootBoardGameRules}.
% TODO: add proper citation in bibliography, not a href

\renewcommand{\arraystretch}{1.5}

% \afterpage{
  % \begin{landscape}
\begin{table}
  \caption{\Marquise \ Flow}
  \label{tab:marquise-flow}
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|p{4cm}|p{8cm}|p{4cm}|} 
    \hline
    \multicolumn{3}{|c|}{\Marquise} \\
    \hline
    Birdsong & Daylight & Evening \\ 
    \hline
    \tabitem Place one wood token at each sawmill. 
  & \tabitem Craft any cards in your hand using workshops.

    \tabitem Take up to three of the following actions (plus one per bird suit card you spend): 
    
      \tabsubitem Battle

      \tabsubitem March: Takes two moves

      \tabsubitem Recruit: Place one warrior at each recruiter.

      \tabsubitem Build: Place one building in a clearing you rule by spending wood tokens equal to its cost.

      \tabsubitem Overwork: Spend a card to place one wood token at one sawmill in a clearing whose suit matches the card spent.
  & \tabitem Draw one card plus one card per draw bonus. (Discard down to five if you have more than five cards.) \\ 
    \hline
    \multicolumn{3}{|c|}{
      \parbox{16cm}{
        \vspace{.5\baselineskip}  
      \textbf{The Keep}: Only you can place pieces in the clearing with the keep token. \\
      \textbf{Field Hospital}: Whenever a Marquise warrior is removed, you may spend a card matching its clearing to move the warrior to the clearing with the keep token.
        \vspace{.5\baselineskip}  
      }
    } \\
    \hline
  \end{tabular}}
\end{table}
\begin{table}
  \caption{\Eyrie \ Flow}
  \label{tab:eyrie-flow}
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|p{4cm}|p{8cm}|p{4cm}|} 
    \hline
    \multicolumn{3}{|c|}{\Eyrie} \\
    \hline
    Birdsong & Daylight & Evening \\ 
    \hline
    \tabitem If your hand is empty, draw 1 card.

    \tabitem Add one or two cards to the Decree.

    \tabitem If you have no roosts, place a roost and 3 warriors in the clearing with the fewest total pieces.
  & \tabitem Craft using roosts.

    \tabitem Resolve the Decree from left column to right, taking one action per card in a matching clearing

      \tabsubitem Recruit: Place warrior in a matching clearing

      \tabsubitem Move
  
      \tabsubitem Battle
  
      \tabsubitem Build: Place roost in a matching clearing you rule 
  & \tabitem Score victory point of rightmost empty space on the Roosts track.

    \tabitem Draw one card plus one card per draw bonus. (Discard down to five if you have more than five cards.) \\ 
    \hline
    \multicolumn{3}{|c|}{
      \parbox{16cm}{
        \vspace{.5\baselineskip}  
      \textbf{Lords of the Forest}: You rule any clearings where you are tied in presence. \\
      \textbf{Disdain for Trade}: When crafting items, you score only 1 victory point.
        \vspace{.5\baselineskip}  
      }
    } \\
    \hline
  \end{tabular}}
\end{table}
  % \end{landscape}
% }

\subsection{Rule difference of \RootV{} and \RootB{}}


In \RootB, \Marquise's Field Hospital ability can be triggered at anytime the \Marquise \ warriors are removed from the board. But in the \RootV, this ability can only be triggered when in \Marquise's turn.

Also, \Marquise \ is not able to choose which wood is spent on building. In \RootV, wood is automatically removed from the board by removing the wood that is closest to the Keep first. 

\section{Game tree}
A \Gls{game-tree} is a directed graph that represents possible game states and decisions within a game. Each node represents a state of the game and each out going edge represents a decision at that state(node) which will lead the another state(node). The size of the game tree can be used to measure the complexity a game as it represents all the possible way a game can pan out. 

A \Gls{complete-game-tree} is a \Gls{game-tree} with all states and decisions. It is possible to generate a \Gls{complete-game-tree} of a perfect information game. Given a \Gls{complete-game-tree}, it is possible to find a series of decisions from our current state that leads to a winning state. 

Root is an imperfect information game, which means generating a \Gls{complete-game-tree} for it is not possible. Therefore, an algorithm that uses the \Gls{complete-game-tree} of Root to find a winning strategy is not possible. But even for Chess and Go, which do have a \Gls{complete-game-tree}, algorithms that play these two games still don't use the \Gls{complete-game-tree} since generating a \Gls{complete-game-tree} for a game with such high complexity as Chess and Go requires such a high amount of memory and therefore is not practical for modern computers. Instead, the algorithms that play these games uses partial game trees.

% TODO: add citations for the Chess and Go algorithms, complexity.

\section{Monte Carlo tree search} % TODO: BK
Monte Carlo tree search (MCTS) is a search algorithm commonly employed in decision processes, particularly within software designed for playing board games. In such application, MCTS is used to solve the \Gls{game-tree}, i.e., identify the decisions that maximizes the probability of leading the player to a winning state. This is done by running many \Gls{playout}s. The result of each \Gls{playout} are then propagated back through the game tree to update the probability of that state to be chosen more, or less. Each round of MCTS consists of 4 steps: Selection, Expansion, Simulation, and Backpropagation.

\subsection{Exploration and exploitation}
During MCTS processes, all nodes have some chance of being randomly picked, which allows new decisions to be made so that new and better outcomes can be discovered. This is called exploration. At the same time, nodes that lead to a winning state will have a higher chance of being picked again in subsequent \Gls{playout}s. This is called exploitation. Maintaining a balance between exploration and exploitation is an important part of implementing decision-making algorithms such as MCTS.

% TODO: add citations to MCTS wikipedia

\section{Reinforcement learning} % TODO: 
Reinforcement learning (RL) is a popular method used in many branches of applications, including game optimization, industry automation, telecommunications, etc. There are 5 main elements of reinforcement learning:

\begin{enumerate}
  \item Agent: the player who makes a decision about what action to take
  \item Environment: the world in which an agent takes actions
  \item State: current situation of the agent
  \item Reward: feedback from the environment
  \item Policy: how agents make decisions
\end{enumerate}

\tikzstyle{block} = [rectangle, rounded corners, 
minimum width=3cm, 
minimum height=1cm,
text centered, 
draw=black]

\tikzstyle{small_block} = [rectangle, rounded corners, 
minimum width=2cm, 
minimum height=1cm,
text centered, 
draw=black]

\tikzstyle{dashed_one_side} = [draw, rectangle, dashed, inner sep=2em]

\tikzstyle{arrow} = [draw, thick, -latex']

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance=2cm]
    \node (env) [block] {Environment};
    \node (agent) [block, below of=env] {Agent};
    \node (poli) [small_block, below of=agent] {Policy};
  
    \begin{scope}
      \draw [arrow] (env)-- ($(env.east)-(-0.5,0)$) |- node[anchor=center, right, pos=0.225] {State, Reward} (agent);
      \draw [arrow] (agent)--($(agent.west)-(0.5,0)$) |- node[anchor=center, left, pos=0.275] {Action} (env);
      \draw [arrow] (poli) -- (agent);
      \draw [arrow] (agent) -- (poli);
    \end{scope}
  \end{tikzpicture}
  
  \caption{Reinforcement Learning Model}
  \label{fig:rl-flow}
\end{figure}

The concept of this method is to use agents to learn in a reward-punishment environment. The reinforcement learning model is described in (Fig. 2.2) The goal of reinforcement learning is to learn a policy that maximizes the reward the agent will receive. At first, the agent will be given the objective they must achieve. At each timestep of learning, the agent will receive the current state and available actions. Then, the agent will decide what actions to take from the policy they have, moving to a new state. By using the reward function, the feedback is returned to the agent to tell how well they behave or how they take actions. The agent will use that feedback to update their policy and then take another action, repeating the process until the agent has reached its objectives.

Reinforcement learning is different from supervised learning in that there is no correct answer in reinforcement learning; instead, the reinforcement agent decides how to carry out the given task. In supervised learning, the training data includes the answer key, so the model is trained with that answer.
 
\section{CPE knowledge used, applied, or integrated in this project} % TODO: BK


% \section{Second section}
% Section 2 text.

% \subsection{Subsection heading goes here}

% Subsection 1 text

% \subsubsection{Subsubsection 1 heading goes here}
% Subsubsection 1 text

% \subsubsection{Subsubsection 2 heading goes here}
% Subsubsection 2 text

% \section{Third section}
% Section 3 text. The dielectric constant\index{dielectric constant}
% at the air-metal interface determines
% the resonance shift\index{resonance shift} as absorption or capture occurs
% is shown in Equation~\eqref{eq:dielectric}:

% \begin{equation}\label{eq:dielectric}
% k_1=\frac{\omega}{c({1/\varepsilon_m + 1/\varepsilon_i})^{1/2}}=k_2=\frac{\omega
% \sin(\theta)\varepsilon_\mathit{air}^{1/2}}{c}
% \end{equation}

% \noindent
% where $\omega$ is the frequency of the plasmon, $c$ is the speed of
% light, $\varepsilon_m$ is the dielectric constant of the metal,
% $\varepsilon_i$ is the dielectric constant of neighboring insulator,
% and $\varepsilon_\mathit{air}$ is the dielectric constant of air.

% \section{About using figures in your report}

% % define a command that produces some filler text, the lorem ipsum.
% \newcommand{\loremipsum}{
%   \textit{Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do
%   eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad
%   minim veniam, quis nostrud exercitation ullamco laboris nisi ut
%   aliquip ex ea commodo consequat. Duis aute irure dolor in
%   reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla
%   pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
%   culpa qui officia deserunt mollit anim id est laborum.}\par}

% \begin{figure}
%   \centering

%   \fbox{
%      \parbox{.6\textwidth}{\loremipsum}
%   }

%   % To include an image in the figure, say myimage.pdf, you could use
%   % the following code. Look up the documentation for the package
%   % graphicx for more information.
%   % \includegraphics[width=\textwidth]{myimage}

%   \caption[Sample figure]{This figure is a sample containing \gls{lorem ipsum},
%   showing you how you can include figures and glossary in your report.
%   You can specify a shorter caption that will appear in the List of Figures.}
%   \label{fig:sample-figure}
% \end{figure}

% Using \verb.\label. and \verb.\ref. commands allows us to refer to
% figures easily. If we can refer to Figures
% \ref{fig:walrus} and \ref{fig:sample-figure} by name in the {\LaTeX}
% source code, then we will not need to update the code that refers to it
% even if the placement or ordering of the figures changes.

% \loremipsum\loremipsum

% % This code demonstrates how to get a landscape table or figure. It
% % uses the package lscape to turn everything but the page number into
% % landscape orientation. Everything should be included within an
% % \afterpage{ .... } to avoid causing a page break too early.
% \afterpage{
%   \begin{landscape}
%   \begin{table}
%     \caption{Sample landscape table}
%     \label{tab:sample-table}

%     \centering

%     \begin{tabular}{c||c|c}
%         Year & A & B \\
%         \hline\hline
%         1989 & 12 & 23 \\
%         1990 & 4 & 9 \\
%         1991 & 3 & 6 \\
%     \end{tabular}
%   \end{table}
%   \end{landscape}
% }

% \loremipsum\loremipsum\loremipsum

% \section{Overfull hbox}

% When the \verb.semifinal. option is passed to the \verb.cpecmu. document class,
% any line that is longer than the line width, i.e., an overfull hbox, will be
% highlighted with a black solid rule:
% \begin{center}
% \begin{minipage}{2em}
% juxtaposition
% \end{minipage}
% \end{center}

% \section{\ifenglish%
% \ifcpe CPE \else ISNE \fi knowledge used, applied, or integrated in this project
% \else%
% ความรู้ตามหลักสูตรซึ่งถูกนำมาใช้หรือบูรณาการในโครงงาน
% \fi
% }

% อธิบายถึงความรู้ และแนวทางการนำความรู้ต่างๆ ที่ได้เรียนตามหลักสูตร ซึ่งถูกนำมาใช้ในโครงงาน

% \section{\ifenglish%
% Extracurricular knowledge used, applied, or integrated in this project
% \else%
% ความรู้นอกหลักสูตรซึ่งถูกนำมาใช้หรือบูรณาการในโครงงาน
% \fi
% }

% อธิบายถึงความรู้ต่างๆ ที่เรียนรู้ด้วยตนเอง และแนวทางการนำความรู้เหล่านั้นมาใช้ในโครงงาน
